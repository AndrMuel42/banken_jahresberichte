{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f753f7c",
   "metadata": {},
   "source": [
    "# Vorbereitung, Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca747232",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt #nur für Testzwecke\n",
    "import re #Regular Expressions importieren, um nach Wortstämmen zu suchen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "527462cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44448d37",
   "metadata": {},
   "source": [
    "# PDF einlesen, leicht vorsäubern und in Datenbank packen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc35312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_fulltext_ubs = []\n",
    "years = [1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020]\n",
    "\n",
    "#Doppelter for-Loop, um jede PDF-Seite in jedem Bericht auszulesen\n",
    "\n",
    "for year in years:\n",
    "    file_path = f\"{year}_ubs_Review_D.pdf\"\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        ubs_file = fitz.open(file_path)\n",
    "        complete_text_ubs = \"\"\n",
    "\n",
    "        for page_num in range(ubs_file.page_count):\n",
    "            page = ubs_file.load_page(page_num)\n",
    "            text = page.get_text()\n",
    "            #Mit Regular Expressions und Replace-Methode mehrfache Leerzeichen, Zeilenumbrüche und Satzzeichen wegbringen\n",
    "            text_without_numbers = re.sub(r'\\d+', '', text)\n",
    "            text_without_linebreaks = text_without_numbers.replace(\"\\n\", \" \")\n",
    "            text_without_linebreaks = text_without_linebreaks.replace(\",\", \" \")\n",
    "            text_without_linebreaks = text_without_linebreaks.replace(\".\", \" \")\n",
    "            text_without_linebreaks = text_without_linebreaks.replace(\"()\", \" \")\n",
    "            text_without_linebreaks = text_without_linebreaks.replace(\"«\", \"\")\n",
    "            text_without_linebreaks = text_without_linebreaks.replace(\"»\", \"\")\n",
    "            text_without_linebreaks = text_without_linebreaks.replace(\":\", \"\")\n",
    "            text_without_linebreaks = re.sub(r\"\\s+\", \" \", text_without_linebreaks).strip()\n",
    "            text_without_linebreaks.split(\" \")\n",
    "            #Über das Zeilenende hinweg getrennte Wörter wieder zusammenfügen\n",
    "            text_without_linebreaks = text_without_linebreaks.replace(\"- \", \"\", )\n",
    "            complete_text_ubs += text_without_linebreaks\n",
    "\n",
    "        list_of_fulltext_ubs.append([year, complete_text_ubs])\n",
    "\n",
    "        ubs_file.close()\n",
    "    else:\n",
    "        print(f\"File not found for year {year}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddda2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas-Dataframe erstellen für die Weiterbearbeitung\n",
    "df = pd.DataFrame(list_of_fulltext_ubs, columns=['Year', 'Text'])\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ea916",
   "metadata": {},
   "source": [
    "# Wortliste erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aeb90b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_list(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a9f57b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Wortliste'] = df['Text'].apply(create_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85cf1e0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to ubs_Review_volltext.csv\n"
     ]
    }
   ],
   "source": [
    "# Zwischenspeicherung als CSV - war nötig, um Fehler beim weiteren Filtern erkennen zu können\n",
    "df.to_csv('ubs_Review_volltext.csv', index=False)\n",
    "\n",
    "print(\"DataFrame saved to ubs_Review_volltext.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd7788d",
   "metadata": {},
   "source": [
    "# Nutzlose Wörter rausfiltern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1152bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(word_list, stopwords):\n",
    "        return [word for word in word_list if word.lower() not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ad1207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords reinladen, Liste dieser Wörter erstellen. Achtung: Immer neuestes Textfile verwenden!\n",
    "with open('stopwords.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5a76762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Wortliste_2'] = df['Wortliste'].apply(lambda x: remove_stopwords(x, stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9bd6cc",
   "metadata": {},
   "source": [
    "# Zahlen rausfiltern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "490404c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zahlenfiltern(word_list):\n",
    "        return [word for word in word_list if not word.isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0037007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Wortliste_2'] = df['Wortliste_2'].apply(lambda x: zahlenfiltern(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e618ad57",
   "metadata": {},
   "source": [
    "# Kommas und Punkte am Wortende rauswerfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0eb0918",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Wortliste_3'] = df['Wortliste_2'].apply(lambda word_list: [word.rstrip(',.:').rstrip('-') for word in word_list])\n",
    "#Hier hat es eine gewisse Doppelung zum Filterschritt im For-Loop drin. Ich behielt den Schritt sicherheitshalber drin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2a029",
   "metadata": {},
   "source": [
    "# Länge der jeweiligen Jahresberichte ergänzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f1b11c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusätzliche Spalte im Dataframe mit der Gesamtlänge des jeweiligen Berichts einfügen, um nachher Häufigkeit zu berechnen\n",
    "df['Total_Length'] = df['Wortliste_3'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c277cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce4615e",
   "metadata": {},
   "source": [
    "# Zählen, wie oft meine Unterbegriffe vorkommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4d1b772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad der Suchbegriffe-Datei festlegen\n",
    "suchbegriffe_file_path = 'suchbegriffe.txt'\n",
    "\n",
    "# Suchbegriffe-Dictionary aus der Datei auslesen\n",
    "with open(suchbegriffe_file_path, 'r', encoding='utf-8') as file:\n",
    "    suchbegriffe_dict = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e75fa8ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Erstelle ein neues Dataframe für die Auswertung\n",
    "result_df = pd.DataFrame(columns=['Year'])\n",
    "\n",
    "# Iteriere durch jedes Jahr im Original-Dataframe\n",
    "for year in df['Year'].unique():\n",
    "    # Extrahiere die relevanten Zeilen für das aktuelle Jahr\n",
    "    year_rows = df[df['Year'] == year]\n",
    "\n",
    "    # Leeres Dictionary für die Häufigkeiten des aktuellen Jahres\n",
    "    year_freqs = {'Year': year}\n",
    "\n",
    "    # Iteriere durch die Suchbegriffe und zähle deren Häufigkeit im aktuellen Jahr\n",
    "    for key, value in suchbegriffe_dict.items():\n",
    "        # Initialisiere ein leeres Dictionary für die Unterbegriffe\n",
    "        subfreqs = {}\n",
    "        \n",
    "        # Iteriere durch die Unterbegriffe und zähle deren Häufigkeit im aktuellen Jahr\n",
    "        for subterm in value:\n",
    "            sub_count = sum(year_rows['Wortliste_3'].apply(lambda words: sum(word.lower() == subterm for word in words)))\n",
    "            subfreqs[subterm] = sub_count\n",
    "        \n",
    "        # Füge die Häufigkeiten der Unterbegriffe zum Ergebnis-Dataframe hinzu\n",
    "        year_freqs.update(subfreqs)\n",
    "    \n",
    "    # Füge die Häufigkeiten des aktuellen Jahres zum Ergebnis-Dataframe hinzu\n",
    "    result_df = pd.concat([result_df, pd.DataFrame([year_freqs])])\n",
    "\n",
    "# Index des Ergebnis-Dataframes resetten\n",
    "result_df = result_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ea09b",
   "metadata": {},
   "source": [
    "# Wordcount der Unterbegriffe aggregieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34eb9dc0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Erstelle ein leeres Dataframe für die Oberbegriffe\n",
    "oberbegriffe_df = pd.DataFrame(columns=list(suchbegriffe_dict.keys()), dtype=int)\n",
    "\n",
    "# Iteriere durch jedes Jahr im Original-Dataframe\n",
    "for year in df['Year'].unique():\n",
    "    # Extrahiere die relevanten Zeilen für das aktuelle Jahr\n",
    "    year_rows = df[df['Year'] == year]\n",
    "\n",
    "    # Initialisiere ein leeres Dictionary für die Oberbegriffe\n",
    "    oberbegriffe_freqs = {key: 0 for key in suchbegriffe_dict.keys()}\n",
    "\n",
    "    # Iteriere durch die Spalten von result_df\n",
    "    for column in result_df.columns:\n",
    "        # Iteriere durch die Unterbegriffe des aktuellen Oberbegriffs\n",
    "        for key, value in suchbegriffe_dict.items():\n",
    "            if column in value:\n",
    "                # Füge den Wordcount dem entsprechenden Oberbegriff hinzu\n",
    "                oberbegriffe_freqs[key] += result_df.loc[result_df['Year'] == year, column].values[0]\n",
    "\n",
    "    # Füge die Häufigkeiten des aktuellen Jahres zu oberbegriffe_df hinzu\n",
    "    oberbegriffe_df = pd.concat([oberbegriffe_df, pd.DataFrame([oberbegriffe_freqs])], ignore_index=True)\n",
    "\n",
    "# Füge die Jahre als eigene Spalte zu oberbegriffe_df hinzu\n",
    "oberbegriffe_df['Year'] = df['Year'].unique()\n",
    "\n",
    "# Ordne die Spalten in oberbegriffe_df neu an\n",
    "oberbegriffe_df = oberbegriffe_df[['Year'] + list(suchbegriffe_dict.keys())]\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "#print(oberbegriffe_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "994e748d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Füge die Gesamtlänge der jeweiligen Jahresberichte auch im Oberbegriffe-Dataframe hinzu\n",
    "oberbegriffe_df[\"Total_Length\"]=df[\"Total_Length\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb34953",
   "metadata": {},
   "source": [
    "# Häufigkeiten berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01711b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Year   Schweiz    Global    Risiko  Nachhaltigkeit     Klima  Diversity  \\\n",
      "0   1999  0.007607  0.008192  0.002487        0.000585  0.000000   0.000439   \n",
      "1   2000  0.009271  0.005343  0.002671        0.000471  0.000000   0.001257   \n",
      "2   2001  0.006513  0.003704  0.002937        0.000383  0.000000   0.001533   \n",
      "3   2002  0.005114  0.006191  0.003634        0.000808  0.000000   0.001077   \n",
      "4   2003  0.005807  0.004051  0.004321        0.000270  0.000000   0.001215   \n",
      "5   2004  0.004755  0.002804  0.005365        0.000488  0.000000   0.000975   \n",
      "6   2005  0.006771  0.003385  0.002176        0.001451  0.000000   0.000725   \n",
      "7   2006  0.005259  0.006206  0.002524        0.000631  0.000000   0.001052   \n",
      "8   2007  0.005457  0.008003  0.003274        0.002546  0.001091   0.000364   \n",
      "9   2008  0.003336  0.005003  0.008339        0.001334  0.000000   0.000000   \n",
      "10  2009  0.007251  0.003296  0.007251        0.000659  0.000000   0.000000   \n",
      "11  2010  0.005053  0.005255  0.003840        0.000606  0.000000   0.000202   \n",
      "12  2011  0.008462  0.007606  0.002757        0.000951  0.000000   0.000570   \n",
      "13  2012  0.007496  0.006082  0.002546        0.001839  0.000000   0.000000   \n",
      "14  2013  0.009502  0.004904  0.002759        0.003525  0.000307   0.000613   \n",
      "15  2014  0.007406  0.005995  0.005114        0.005114  0.000882   0.000176   \n",
      "16  2015  0.007732  0.004087  0.004087        0.005192  0.000221   0.000442   \n",
      "17  2016  0.004767  0.001969  0.005699        0.004560  0.000207   0.000415   \n",
      "18  2017  0.005568  0.006917  0.004555        0.003712  0.001181   0.000506   \n",
      "19  2018  0.004927  0.005151  0.002688        0.007615  0.000000   0.000224   \n",
      "20  2019  0.005717  0.006432  0.005955        0.018580  0.001906   0.001191   \n",
      "21  2020  0.005149  0.007337  0.003604        0.008753  0.000772   0.002188   \n",
      "\n",
      "     Steuern  Bankgeheimnis       USA  ...  Demut     Stolz  Bescheidenheit  \\\n",
      "0   0.002779            0.0  0.002341  ...    0.0  0.000146        0.000000   \n",
      "1   0.002671            0.0  0.004871  ...    0.0  0.000471        0.000000   \n",
      "2   0.002043            0.0  0.003704  ...    0.0  0.000000        0.000128   \n",
      "3   0.003230            0.0  0.005114  ...    0.0  0.000135        0.000000   \n",
      "4   0.002701            0.0  0.007427  ...    0.0  0.000135        0.000000   \n",
      "5   0.001585            0.0  0.005365  ...    0.0  0.000000        0.000000   \n",
      "6   0.002055            0.0  0.005803  ...    0.0  0.000121        0.000121   \n",
      "7   0.001473            0.0  0.004102  ...    0.0  0.000105        0.000000   \n",
      "8   0.002183            0.0  0.005457  ...    0.0  0.000364        0.000000   \n",
      "9   0.001334            0.0  0.005670  ...    0.0  0.000000        0.000000   \n",
      "10  0.002307            0.0  0.006922  ...    0.0  0.000000        0.000000   \n",
      "11  0.001819            0.0  0.003436  ...    0.0  0.000000        0.000000   \n",
      "12  0.000856            0.0  0.003613  ...    0.0  0.000095        0.000000   \n",
      "13  0.000283            0.0  0.002122  ...    0.0  0.000000        0.000000   \n",
      "14  0.000766            0.0  0.002299  ...    0.0  0.000307        0.000000   \n",
      "15  0.001234            0.0  0.002292  ...    0.0  0.000529        0.000000   \n",
      "16  0.000552            0.0  0.002209  ...    0.0  0.000110        0.000000   \n",
      "17  0.001244            0.0  0.001658  ...    0.0  0.000000        0.000000   \n",
      "18  0.002025            0.0  0.002868  ...    0.0  0.000337        0.000000   \n",
      "19  0.000896            0.0  0.001568  ...    0.0  0.000000        0.000000   \n",
      "20  0.001191            0.0  0.000476  ...    0.0  0.000715        0.000000   \n",
      "21  0.000386            0.0  0.003347  ...    0.0  0.000257        0.000000   \n",
      "\n",
      "    Liquidität     Basel    Zürich    London  New York     Ruhen  Total_Length  \n",
      "0     0.000146  0.000731  0.000878  0.001463  0.001317  0.000000          6836  \n",
      "1     0.000000  0.000629  0.000314  0.000471  0.000471  0.000000          6364  \n",
      "2     0.000000  0.000766  0.001533  0.000639  0.001405  0.000000          7830  \n",
      "3     0.000000  0.000808  0.001211  0.000808  0.001750  0.000000          7430  \n",
      "4     0.000000  0.000540  0.000945  0.001080  0.001891  0.000000          7405  \n",
      "5     0.000122  0.000488  0.000975  0.001219  0.002073  0.000000          8202  \n",
      "6     0.000242  0.001209  0.002176  0.002176  0.002539  0.000000          8271  \n",
      "7     0.000000  0.000841  0.000841  0.001367  0.002735  0.000000          9507  \n",
      "8     0.000364  0.000000  0.001091  0.000364  0.002183  0.000000          2749  \n",
      "9     0.000000  0.000334  0.001001  0.000334  0.002001  0.000000          2998  \n",
      "10    0.000330  0.000000  0.000989  0.000330  0.001648  0.000000          3034  \n",
      "11    0.000404  0.000606  0.001819  0.000404  0.000808  0.001415          4948  \n",
      "12    0.000380  0.002852  0.000951  0.000666  0.000951  0.000095         10518  \n",
      "13    0.000424  0.002546  0.001839  0.000849  0.001414  0.000000          7070  \n",
      "14    0.000307  0.002146  0.001226  0.000307  0.000920  0.000000          6525  \n",
      "15    0.000000  0.000882  0.000882  0.000176  0.001058  0.000000          5671  \n",
      "16    0.000110  0.000884  0.000331  0.000221  0.000442  0.000000          9053  \n",
      "17    0.000000  0.000207  0.000622  0.000000  0.000104  0.000000          9650  \n",
      "18    0.000337  0.000000  0.001012  0.000506  0.000675  0.000000          5927  \n",
      "19    0.000224  0.000000  0.000896  0.000896  0.000224  0.000000          4465  \n",
      "20    0.000238  0.000476  0.000953  0.001191  0.001191  0.000000          4198  \n",
      "21    0.000386  0.000386  0.000644  0.000000  0.000772  0.000000          7769  \n",
      "\n",
      "[22 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "# Erstelle ein leeres Dataframe für die relative Häufigkeit der Oberbegriffe\n",
    "oberbegriffe_häufigkeit_df = oberbegriffe_df.copy()\n",
    "\n",
    "# Iteriere durch jedes Jahr im Dataframe\n",
    "for year in oberbegriffe_df['Year']:\n",
    "    # Extrahiere die Total Length des aktuellen Jahres\n",
    "    total_length = oberbegriffe_df.loc[oberbegriffe_df['Year'] == year, 'Total_Length'].values[0]\n",
    "    \n",
    "    # Iteriere durch die Oberbegriffe\n",
    "    for key in suchbegriffe_dict.keys():\n",
    "        # Berechne die relative Häufigkeit und setze den Wert in das neue Dataframe\n",
    "        oberbegriffe_häufigkeit_df.loc[oberbegriffe_häufigkeit_df['Year'] == year, key] /= total_length\n",
    "\n",
    "# Ergebnis anzeigen\n",
    "print(oberbegriffe_häufigkeit_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef5073e",
   "metadata": {},
   "source": [
    "# CSV-Export für die grafische Auswertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bca20c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame saved to Häufigkeit_UBS_Begriffe.csv\n"
     ]
    }
   ],
   "source": [
    "# Dataframe exportieren in eine CSV-Datei, für die weitere Analyse und grafische Auswertung\n",
    "oberbegriffe_häufigkeit_df.to_csv('Häufigkeit_UBS_Begriffe.csv', index=False)\n",
    "\n",
    "print(\"Updated DataFrame saved to Häufigkeit_UBS_Begriffe.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
